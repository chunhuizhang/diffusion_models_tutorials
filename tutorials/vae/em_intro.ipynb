{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73586131-52f3-45c4-a63e-a868e5687fd9",
   "metadata": {},
   "source": [
    "> amortized inference\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/368959795"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9dbf9f-9c2c-49bd-b285-3ac5344a24be",
   "metadata": {},
   "source": [
    "- E-step: 取 $q(z)=p(z|x)$，此时 $kl(q(z)\\|p(z|x))=0$\n",
    "- M-step: $\\arg\\max \\int q(z)\\log p(x,z)dz$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b2175-4352-454c-8eca-d6e2fed36e51",
   "metadata": {},
   "source": [
    "### EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2962d-b92f-4082-b4a7-e8f4ab2e7647",
   "metadata": {},
   "source": [
    "> EM算法的精妙之处在于，它保证了每一步迭代都会使观测数据的似然函数 $L(\\theta)$ 单调不减，即 $L(\\theta^{(t+1)}) \\ge L(\\theta^{(t)})$\n",
    "\n",
    "- 变量及符号\n",
    "    - $X = \\{x_1, x_2, \\dots, x_m\\}$：观测数据 (Observed Data)，我们能看到的数据（如身高列表）。\n",
    "    - $Z = \\{z_1, z_2, \\dots, z_m\\}$：隐变量 (Latent Variable)，我们无法观测的数据（如性别列表）。\n",
    "    - $(X, Z)$：完整数据 (Complete Data)，包含了观测数据和隐变量。\n",
    "    - $\\theta = \\{\\mu_M, \\sigma_M^2, \\mu_F, \\sigma_F^2, \\pi_M, \\pi_F\\}$：模型参数 (Parameters)，这是我们想要估计的目标\n",
    "- 目标\n",
    "    - $ L(\\theta) = \\log P(X|\\theta) =  \\log \\sum_{Z} P(X, Z | \\theta) $\n",
    "        - 对数里面带着一个求和项，这在求导和优化时非常困难\n",
    "- EM：EM算法通过引入一个辅助的Q函数来巧妙地解决这个问题。(假设在第 $t$ 次迭代时，我们拥有的参数是 $\\theta^{(t)}$)\n",
    "    - E-Step (Expectation Step):\n",
    "        - 计算完整数据对数似然 $\\log P(X, Z | \\theta)$ 关于 $P(Z|X, \\theta^{(t)})$ 的期望。这个期望函数被称为 Q函数。\n",
    "        - $Q(\\theta | \\theta^{(t)}) = E_{Z|X, \\theta^{(t)}}[\\log P(X, Z | \\theta)]= \\sum_{Z} P(Z|X, \\theta^{(t)}) \\log P(X, Z | \\theta) $\n",
    "            - $P(Z|X, \\theta^{(t)})$ 是在给定当前观测数据 $X$ 和上一轮参数 $\\theta^{(t)}$ 的条件下，隐变量 $Z$ 的后验概率。在我们的身高例子里，这就是计算每个身高 $x_i$，属于某个性别 $z_i$ 的概率。这通常是E步中实际需要计算的部分。\n",
    "            - $\\log P(X, Z | \\theta)$ 是完整数据的对数似然。因为 $Z$ 已知，这个表达式通常很简单（比如，如果知道性别，计算高斯分布的概率就很容易）。\n",
    "        - $ Q(\\theta | \\theta^{(t)}) = \\sum_{i=1}^{m} \\sum_{z_i} P(z_i|x_i, \\theta^{(t)}) \\log P(x_i, z_i | \\theta) $\n",
    "    - M-Step (Maximization Step):\n",
    "        - $ \\theta^{(t+1)} = \\arg\\max_{\\theta} Q(\\theta | \\theta^{(t)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6856378-2d89-4c4b-b9e7-03fe0657df4f",
   "metadata": {},
   "source": [
    "### EM 计算示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0997a188-1c20-4542-84d2-30eba04248b1",
   "metadata": {},
   "source": [
    "- https://www.youtube.com/watch?v=3zbAsgCf1Sw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070fac5-041e-4a15-9860-73cefb8fc25f",
   "metadata": {},
   "source": [
    "- 情况一：已知分布，未知数据\n",
    "    - `[1, 2, x]` 这三个样本是从一个均值为 1、方差为 1 的正态分布 $N(1, 1)$ 中抽取的。那么，对于缺失数据 x 的最佳猜测是什么？\n",
    "    - 因为我们知道分布的均值是 1，所以对 x 的最佳猜测就是其期望值，即 x = 1。\n",
    "- 情况二：数据完整，未知参数\n",
    "    - 我告诉你 `[0, 1, 2]` 这三个样本是从一个均值为 $\\mu$、方差为 1 的正态分布 $N(\\mu, 1)$ 中抽取的。那么，对于未知参数 $\\mu$ 的最佳猜测是什么？\n",
    "    - 在这种情况下，$\\mu$ 的最大似然估计就是样本均值，即 $\\mu = \\frac{0+1+2}{3} = 1$。\n",
    "- 情况三：数据不完整且参数未知（EM 算法的核心问题）\n",
    "    - 我告诉你 `[1, 2, x]` 这三个样本是从一个均值为 $\\mu$、方差为 1 的正态分布 $N(\\mu, 1)$ 中抽取的。那么，对于缺失数据 x 和未知参数 $\\mu$ 的最佳猜测是什么？\n",
    "    - 我们既不知道参数 $\\mu$，也不知道缺失数据 x。如果我们知道 $\\mu$，就可以像情况一那样估计 x。如果我们知道 x，就可以像情况二那样估计 $\\mu$。这就形成了一个“鸡生蛋，蛋生鸡”的困境，而 EM 算法正是为了解决这类问题而设计的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89f475-2101-46f8-934a-0a9486b5b66f",
   "metadata": {},
   "source": [
    "- 似然函数 (Likelihood Function)\n",
    "    - 假设数据点服从正态分布 $p(x|\\mu) \\sim N(\\mu, 1)$。对于包含缺失数据的完整数据集 (x, 1, 2)，其似然函数可以表示为：\n",
    "        - $L(x, 1, 2 | \\mu) = p(x|\\mu) \\times p(1|\\mu) \\times p(2|\\mu)$\n",
    "- EM 算法通过迭代的方式交替进行以下两步，直到收敛：\n",
    "    - 初始化 (Initialization): 对未知参数 $\\mu$ 进行一个初始猜测，记为 $\\mu_0$。\n",
    "    - E-步 (Expectation Step): 基于当前的参数估计 $\\mu_0$，计算完整数据对数似然函数关于缺失数据 x 的期望。\n",
    "        - $E[\\log L | \\mu_0] = \\int p(x|\\mu_0) \\log L(x, 1, 2 | \\mu) dx$\n",
    "    - M-步 (Maximization Step): 最大化在 E-步中计算出的期望，以更新参数的估计值，得到 $\\mu_1$。\n",
    "        - $\\mu_1 = \\underset{\\mu}{\\operatorname{argmax}} E[\\log L | \\mu_0]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c8264-1743-4a15-8a6f-d12855ddd6c8",
   "metadata": {},
   "source": [
    "- 第 0 步 (初始化): 随机猜测一个 $\\mu$ 的初始值，例如 $\\mu_0 = 0$。\n",
    "- 第 1 步 (类 E-步): 基于 $\\mu_0 = 0$，我们对缺失数据 x 的最佳猜测是其期望值，即 $x_0 = 0$。\n",
    "- 第 2 步 (类 M-步): 现在我们有了一个“完整”的数据集 [1, 2, 0]。我们基于这个数据集来更新 $\\mu$ 的估计值，即 $\\mu_1 = \\frac{1+2+0}{3} = 1$。\n",
    "- 第 3 步 (类 E-步): 基于新的参数 $\\mu_1 = 1$，我们再次更新对 x 的猜测，得到 $x_1 = 1$。\n",
    "- 第 4 步 (类 M-步): 基于新的“完整”数据集 [1, 2, 1]，我们再次更新 $\\mu$，得到 $\\mu_2 = \\frac{1+2+1}{3} = \\frac{4}{3}$。\n",
    "- ... 这个过程持续进行，$\\mu$ 和 x 的值会不断更新，并逐渐逼近一个稳定点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e3275f-1c6a-4023-9eb9-b7f3f271b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ade605-c155-433e-bcac-eb923dacc2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n",
      "1 1.3333333333333333\n",
      "2 1.4444444444444444\n",
      "3 1.4814814814814816\n",
      "4 1.4938271604938274\n",
      "5 1.4979423868312758\n",
      "6 1.4993141289437586\n",
      "7 1.4997713763145863\n",
      "8 1.4999237921048623\n",
      "9 1.4999745973682874\n"
     ]
    }
   ],
   "source": [
    "mu = 0\n",
    "for i in range(10):\n",
    "    x = mu\n",
    "    mu = np.mean([1, 2, x])\n",
    "    print(i, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8aa5e7-5a96-415c-aad6-61d807c61846",
   "metadata": {},
   "source": [
    "- 当算法收敛时，参数和缺失数据的估计值将达到一个平衡状态，不再发生变化。在这个稳定点上，以下两个条件必须同时满足：\n",
    "    - 参数 $\\mu$ 是当前完整数据的样本均值：$\\mu=\\frac{1+2+x}{3}$\n",
    "    - 缺失数据 x 的最佳猜测是当前的参数 $\\mu$：$x=\\mu$\n",
    "- 将第二个方程代入第一个方程，我们可以解出收敛时的值 $\\mu^*$ 和 $x^*$：\n",
    "    - $\\mu^* = \\frac{1+2+\\mu^*}{3}$\n",
    "    - $x^* = \\mu^* = 1.5$。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
