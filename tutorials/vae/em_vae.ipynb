{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d02fd8f-cc4f-4011-bebd-68650d717980",
   "metadata": {},
   "source": [
    "- VAE 可以被理解为 EM 算法在处理复杂模型（尤其是深度神经网络）时的一种近似和推广。它们的核心思想一脉相承，都是为了解决含隐变量模型的最大似然估计问题。\n",
    "- 无论是 EM 还是 VAE，它们的目标都是最大化观测数据的对数似然 $\\log p(x)$。当模型包含隐变量 $z$ 时，这个似然函数因为需要对 $z$ 进行积分（$\\log ∫ p(x,z)dz$）而变得难以直接优化。\n",
    "    - 为了解决这个问题，两者都引入了一个共同的工具——证据下界（Evidence Lower Bound, ELBO）。\n",
    "    - ELBO 是 $\\log p(x; θ)$ 的一个下界。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1d452-3181-4045-bcf6-a82e9dc1b0f6",
   "metadata": {},
   "source": [
    "| 特征 | EM 算法 (Expectation-Maximization) | VAE (Variational Autoencoder) |\n",
    "| :--- | :--- | :--- |\n",
    "| **核心目标** | 在有隐变量的情况下，最大化数据的对数似然 `log p(x)`。 | 同样是最大化 `log p(x)`，但实际操作是最大化其下界 (ELBO)。 |\n",
    "| **隐变量 `z`** | 通常是低维、离散的（如：聚类的类别）。 | 通常是高维、连续的（如：图像的特征向量）。 |\n",
    "| **E-Step (推断)** | **精确计算**后验概率 `p(z\\|x)`。要求模型简单，后验可解。 | 用一个**编码器网络 `q(z\\|x)`** 来**近似**不可解的后验 `p(z\\|x)`。 |\n",
    "| **M-Step (学习)** | 根据 E-step 的结果，用**解析解或简单优化**来更新模型参数 `p(x\\|z)`。 | 用一个**解码器网络**来代表 `p(x\\|z)`，通过**梯度下降**更新其参数。 |\n",
    "| **执行方式** | E-step 和 M-step **交替迭代**，直到收敛。 | 编码器和解码器**联合训练**，通过梯度下降进行端到端优化。 |\n",
    "| **本质** | 一种用于含有隐变量的参数估计的**迭代算法**。 | 一种用于复杂数据建模和生成的**深度生成模型**。 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
