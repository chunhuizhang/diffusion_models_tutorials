{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b1cb24-f1ee-49fc-b309-1768c2b7a864",
   "metadata": {},
   "source": [
    "- resources\n",
    "    - https://github.com/AntixK/PyTorch-VAE/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94061a21-4516-44cb-bc0b-f8607367b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd634eaa-865f-474b-91c3-71a02017e225",
   "metadata": {},
   "source": [
    "- posterior inference：$p_\\theta(z|x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f48c4c3-8744-4ab8-b0a0-4efbcb553b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://imgur-backup.hackmd.io/4RITZyB.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://imgur-backup.hackmd.io/4RITZyB.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848c742-738f-4ccd-97d8-a99c613ad3ba",
   "metadata": {},
   "source": [
    "- Instead of mapping the input $x$ directly into $z$, we instead map $x$ into two vectors: $z_\\mu ∈ ℝ^n$ & $z_{\\sigma^2} ∈ ℝ^n$.\n",
    "    - These two vectors are used to parametrize a Gaussian normal distribution from which we sample the latent vector z: $z\\sim\\mathcal N(z_\\mu, z_{\\sigma^2})$. This makes our encoder variational (**probabilistic**), basically adding gaussian noise to our encoder models output vector z.\n",
    "    - Why should we add noise to the encoder? Doing so will generate many more different sample points of z for the decoder to learn reconstructions from, forcing the decoder to generate smooth interpolations between local samples in the latent space.\n",
    "    - Computing the derivatives of the random gaussian distribution parameterized by the two output vectors z_mean and z_var of the encoder is achieved by reparameterizing z ~ gauss(z_mean, z_var) into z = z_mean + z_var * gauss(0,1)\n",
    "        - This so called reparameterization trick enables us to take the derivatives of z with respect to either z_mean or z_var, which are necessary to back-propagate the error-signal through the sampling layer when using stochastic gradient descent as the parameter optimizer.\n",
    "- To prevent the variational encoder of “cheating” by placing different samples far apart from each other in z (avoiding our desired property of smooth local interpolations) we add an additional loss term to our reconstruction loss function L(x, x’), namely: KL(gauss(z_mean, z_var) || gauss(0,1)). This additional loss term is defined as the Kullback-Leibler-divergence between the encoders output gauss(z_mean, z_var) and an isotropic standard normal distribution gauss(0,1) ∈ ℝ^n forcing the latent space to be standard Gaussian distributed (achieving the desired smooth local interpolation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f9b26-478f-4d06-ab33-7bc49120b3f6",
   "metadata": {},
   "source": [
    "### ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a0964d-bd6b-492c-87cf-6ecc9175a152",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Evidence_lower_bound\n",
    "- 两个最常用的 formula\n",
    "    - $q_\\phi(z|x)$ 是 encoder，$p_\\theta(x|z)$ 是 decoder\n",
    "    - $\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]$ 刻画的是重构；\n",
    "    - $D_{\\text{KL}}(q_\\phi(z|x)||p(z))$ 刻画的是 $z$ 空间的正则；\n",
    "\n",
    "$$\n",
    "\\log p(x) \\geq \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x)||p(z))\n",
    "$$\n",
    "$$\n",
    "L(\\phi, \\theta; x) := \\mathbb E_{z\\sim q_\\phi(\\cdot | x)} \\left[ \\ln\\frac{p_\\theta(x,  z)}{q_\\phi(z|x)} \\right]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
