{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29061294-14c6-4c3e-97d3-14c166a3a211",
   "metadata": {},
   "source": [
    "### kld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa53256-cd1d-422e-8986-4f04d19d1aae",
   "metadata": {},
   "source": [
    "```python\n",
    "kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "```\n",
    "\n",
    "-  $D_{KL}(Q(z|X) || P(z))$ where P(z) is N(0, I)\n",
    "    -  $-\\frac12\\sum (1+\\log\\sigma^2-\\mu^2-\\sigma^2)$\n",
    "    -  对角高斯分布 (Diagonal Gaussian): 为了简化计算，我们做一个重要假设：隐空间的各个维度之间是相互独立的。这意味着描述这个多维高斯分布的协方差矩阵是一个对角矩阵（只有对角线上有值，其余都为0）。\n",
    "    -  $D_{KL}( \\mathcal{N}(\\mu, \\sigma^2 I) \\ || \\ \\mathcal{N}(0, I))$\n",
    "        -  两个 $d$ 维高斯分布，$P_1 = \\mathcal{N}(\\mu_1, \\Sigma_1)$, $P_2 = \\mathcal{N}(\\mu_2, \\Sigma_2)$\n",
    "        -  $D_{KL}(P_1 || P_2) = \\frac{1}{2} \\left( \\text{tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2 - \\mu_1)^T \\Sigma_2^{-1} (\\mu_2 - \\mu_1) - d + \\ln\\left(\\frac{\\det \\Sigma_2}{\\det \\Sigma_1}\\right) \\right)$\n",
    "        - 我们这里的 $\\mu_1 = \\mu, \\Sigma_1 = \\text{diag}(\\sigma_1^2, ..., \\sigma_d^2) = \\sigma^2 I$，$\\mu_2 = 0, \\Sigma_2 = I$\n",
    "        - $D_{KL} = \\frac{1}{2} \\sum_{i=1}^{d} (\\sigma_i^2 + \\mu_i^2 - 1 - \\ln(\\sigma_i^2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24eb9c9-e191-4106-b2c9-2f85158c0ac1",
   "metadata": {},
   "source": [
    "### reparameterize trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce93804-220c-4101-b05f-c8f2b81a9545",
   "metadata": {},
   "source": [
    "```python\n",
    "def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param mu: (Tensor) Mean of the latent Gaussian\n",
    "    :param logvar: (Tensor) Standard deviation of the latent Gaussian\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps * std + mu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a5236-e92d-4d2a-a6de-30abd4739829",
   "metadata": {},
   "source": [
    "### loss scale??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0a15e-e498-46a6-b478-309b50a43f8f",
   "metadata": {},
   "source": [
    "```python\n",
    "reconstruction_loss = F.mse_loss(x_hat, x, reduction='mean')\n",
    "```\n",
    "$$\n",
    "L_{\\text{MSE}} = \\frac{1}{B \\cdot C \\cdot H \\cdot W} \\sum_{b=1}^{B} \\sum_{c=1}^{C} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{b, c, h, w} - \\hat{x}_{b, c, h, w})^2\n",
    "$$\n",
    "\n",
    "- kld\n",
    "    - 先对潜在维度求和，再对批次维度求均值。这计算了每个数据点的平均KL散度。\n",
    "\n",
    "```python\n",
    "kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1) # 对每个数据点的潜变量维度求和\n",
    "kl_loss = torch.mean(kl_loss) # 对batch维度取均值\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf542f3-e07f-4337-a4ba-59eb61607029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. 定义我们的输入张量 (tensors)\n",
    "# 形状为 (B, C, H, W) = (2, 1, 1, 2)\n",
    "x = torch.tensor([[[[1.0, 2.0]]], \n",
    "                  [[[3.0, 4.0]]]], dtype=torch.float32)\n",
    "\n",
    "x_hat = torch.tensor([[[[1.5, 2.5]]], \n",
    "                      [[[2.0, 5.0]]]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c5ad3d-3a1c-4a1d-ba39-bfadd8de121d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19174622-2707-4389-b177-6bce8b5a1cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6250)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(x_hat, x, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c55ef65-58c5-497c-882d-c01d18298a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6250)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((x_hat - x) ** 2).sum() / x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce243296-295a-41a9-ab78-f68ed25dc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def vae_loss_function(x_hat, x, mu, log_var, beta=1.0):\n",
    "    recon_loss = F.mse_loss(x_hat, x, reduction='mean')\n",
    "\n",
    "    # 2. KL 损失 (KL Divergence Loss)\n",
    "    # D_KL(q(z|x) || p(z)) = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # 首先，对每个数据点的潜变量维度求和 (dim=1)\n",
    "    # 然后，对batch维度取均值\n",
    "    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1)\n",
    "    kl_loss = torch.mean(kl_div, dim=0)\n",
    "\n",
    "    # 3. 总损失\n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "\n",
    "    return total_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0878a622-0779-4bbe-b830-e29863023be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta: 1\n",
      "Reconstruction Loss (per-pixel mean): 0.1667\n",
      "KL Loss (per-datapoint mean): 8.3359\n",
      "Weighted KL Loss: 8.3359\n",
      "Total Loss: 8.5025\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "latent_dim = 10\n",
    "img_channels = 1\n",
    "# img_size = 28\n",
    "img_size = 64*64\n",
    "\n",
    "# 模拟模型输出\n",
    "x = torch.rand(batch_size, img_channels, img_size, img_size)\n",
    "x_hat = torch.rand(batch_size, img_channels, img_size, img_size)\n",
    "mu = torch.randn(batch_size, latent_dim)\n",
    "log_var = torch.randn(batch_size, latent_dim)\n",
    "\n",
    "# 设置 beta 值 (可以从一个较小的值开始，或者使用KL退火)\n",
    "beta_value = 1 \n",
    "\n",
    "total_loss, recon_loss, kl_loss = vae_loss_function(x_hat, x, mu, log_var, beta=beta_value)\n",
    "\n",
    "print(f\"Beta: {beta_value}\")\n",
    "print(f\"Reconstruction Loss (per-pixel mean): {recon_loss.item():.4f}\")\n",
    "print(f\"KL Loss (per-datapoint mean): {kl_loss.item():.4f}\")\n",
    "print(f\"Weighted KL Loss: {(beta_value * kl_loss).item():.4f}\")\n",
    "print(f\"Total Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df1200-7fa0-41e3-be7c-1f17d6e3cd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
